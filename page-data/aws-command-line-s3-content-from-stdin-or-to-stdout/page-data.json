{"componentChunkName":"component---src-templates-blog-post-js","path":"/aws-command-line-s3-content-from-stdin-or-to-stdout/","result":{"data":{"site":{"siteMetadata":{"title":"Luciano Mammino \"Loige\" - Cloud developer, entrepreneur, fighter, butterfly maker!","author":"Luciano Mammino","siteUrl":"https://loige.co","twitterProfile":"loige","disqusShortName":"loige"}},"markdownRemark":{"id":"797f7b9f-2cca-5572-87e4-2a0a08d8398e","fileAbsolutePath":"/home/circleci/loige.co/content/posts/2018-05-05_aws-command-line-s3-content-from-stdin-or-to-stdout/index.md","excerpt":"This article presents a quick tip that will help you deal with the content of files in S3 through the AWS command line in a much faster and…","timeToRead":6,"headings":[{"value":"Some examples","depth":2},{"value":"The “magic” - option in aws cp","depth":2},{"value":"Writing to S3 from the standard output","depth":2},{"value":"Using data from S3 as input for other commands","depth":2},{"value":"Pipeline processing of S3 files","depth":2},{"value":"The 5GB caveat","depth":2},{"value":"That’s all folks","depth":2}],"html":"<p>This article presents a quick tip that will help you deal with the content of files in S3 through the AWS command line in a much faster and simpler way.</p>\n<p>Did you ever want to simply print the content of a file in S3 from your command line and maybe pipe the output to another command? Or maybe, did you ever needed to pipe the standard output of a sequence of commands directly into a file in S3? I had this need multiple times and, before my amazing colleague Paul made me discover the tip I am about to describe here, I was always using intermediary files to keep track of the input and output of S3 files.</p>\n<h2 id=\"some-examples\" style=\"position:relative;\"><a href=\"#some-examples\" aria-label=\"some examples permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Some examples</h2>\n<p>Let’s make few practical examples to make this use case easier to grasp.</p>\n<p>Imagine you have a PostgreSQL database containing GeoIP data and you want to dump all the data to a CSV, gzip it and store it an S3 bucket.</p>\n<p>This is how I used to solve this problem:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token comment\"># dump the data from PostgreSQL to a compressed csv</span>\npsql -U user -d db_name -c <span class=\"token string\">\"Copy (Select * From geoip_v4) To STDOUT With CSV HEADER DELIMITER ',';\"</span> <span class=\"token operator\">|</span> <span class=\"token function\">gzip</span> <span class=\"token operator\">></span> geoip_v4_data.csv.gz\n<span class=\"token comment\"># upload the resulting file to S3</span>\naws s3 <span class=\"token function\">cp</span> geoip_v4_data.csv.gz s3://my-amazing-bucket/geoip_v4_data.csv.gz</code></pre></div>\n<p>At some point in the future, you probably want to read the file from S3 and search for a given CIDR in the content of the file. Again, this is how I would have solved this problem:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token comment\"># download the file from S3</span>\naws s3 <span class=\"token function\">cp</span> s3://my-amazing-bucket/geoip_v4_data.csv.gz <span class=\"token builtin class-name\">.</span>\n<span class=\"token comment\"># decompress the file and search inside it</span>\ngunzip -c geoip_v4_data.csv.gz <span class=\"token operator\">|</span> <span class=\"token function\">grep</span> <span class=\"token string\">\"1.0.8.0/21\"</span></code></pre></div>\n<p>In both cases, I am creating intermediary files and, as you probably already know, this is not ideal for many reasons. Just to name few, this is a slower operation (not fully stream-able), it takes extra space on disk (imagine you have to deal with very big files), finally, it also needs an extra command. Wouldn’t it be great if we could solve both problems by writing a single pipeline of commands?</p>\n<h2 id=\"the-magic---option-in-aws-cp\" style=\"position:relative;\"><a href=\"#the-magic---option-in-aws-cp\" aria-label=\"the magic   option in aws cp permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>The “magic” <code>-</code> option in <code>aws cp</code></h2>\n<p>Buried at the very bottom of the <code>aws s3 cp</code> command help you might (by accident) find this:</p>\n<div class=\"gatsby-highlight\" data-language=\"none\"><pre class=\"language-none\"><code class=\"language-none\">Uploading a local file stream to S3\n\n  WARNING:: PowerShell may alter the encoding of or add a CRLF  to  piped\n  input.\n\n  The  following  cp  command  uploads  a local file stream from standard\n  input to a specified bucket and key:\n\n    aws s3 cp - s3://mybucket/stream.txt\n\nDownloading an S3 object as a local file stream\n\n  WARNING:: PowerShell may alter the encoding of or add a CRLF  to  piped\n  or redirected output.\n\n  The  following cp command downloads an S3 object locally as a stream to\n  standard output. Downloading as a stream is  not  currently  compatible\n  with the --recursive parameter:\n\n    aws s3 cp s3://mybucket/stream.txt -</code></pre></div>\n<p>To make it simple, when running <code>aws s3 cp</code> you can use the special argument <code>-</code> to indicate the content of the standard input or the content of the standard output (depending on where you put the special argument).</p>\n<h2 id=\"writing-to-s3-from-the-standard-output\" style=\"position:relative;\"><a href=\"#writing-to-s3-from-the-standard-output\" aria-label=\"writing to s3 from the standard output permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Writing to S3 from the standard output</h2>\n<p>Using this newly acquired piece of knowledge, we now know we can do something like this to write content from the standard output directly to a file in S3:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">cat</span> <span class=\"token string\">\"hello world\"</span> <span class=\"token operator\">|</span> aws s3 <span class=\"token function\">cp</span> - s3://some-bucket/hello.txt</code></pre></div>\n<p>This way we can rewrite the solution to our first problem as follows:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">psql -U user -d db_name -c <span class=\"token string\">\"Copy (Select * From geoip_v4) To STDOUT With CSV HEADER DELIMITER ',';\"</span> <span class=\"token operator\">|</span> <span class=\"token function\">gzip</span> <span class=\"token operator\">|</span> aws s3 <span class=\"token function\">cp</span> - s3://my-amazing-bucket/geoip_v4_data.csv.gz</code></pre></div>\n<p>This time no intermediary file is created and the data from the gzipped file is immediately streamed to S3 as soon as the first bytes start to be available.</p>\n<h2 id=\"using-data-from-s3-as-input-for-other-commands\" style=\"position:relative;\"><a href=\"#using-data-from-s3-as-input-for-other-commands\" aria-label=\"using data from s3 as input for other commands permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Using data from S3 as input for other commands</h2>\n<p>The magic <code>-</code> argument can be used also to read the content of files in s3 and pass it in the standard output, for instance, you could do the following:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">aws s3 <span class=\"token function\">cp</span> s3://some-bucket/hello.txt -</code></pre></div>\n<p>This will output:</p>\n<div class=\"gatsby-highlight\" data-language=\"none\"><pre class=\"language-none\"><code class=\"language-none\">hello world</code></pre></div>\n<p>Let’s use this option to rewrite the solution to our second problem as a one-liner:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">aws s3 <span class=\"token function\">cp</span> s3://my-amazing-bucket/geoip_v4_data.csv.gz - <span class=\"token operator\">|</span> gunzip -c geoip_v4_data.csv.gz <span class=\"token operator\">|</span> <span class=\"token function\">grep</span> <span class=\"token string\">\"1.0.8.0/21\"</span></code></pre></div>\n<p>This approach looks much similar to what you would do with a local file and makes integrating other commands seamless with the content of files available in your S3 storage.</p>\n<h2 id=\"pipeline-processing-of-s3-files\" style=\"position:relative;\"><a href=\"#pipeline-processing-of-s3-files\" aria-label=\"pipeline processing of s3 files permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Pipeline processing of S3 files</h2>\n<p>We can combine the learnings from the previous two sections to build processing pipelines for S3 files.</p>\n<p>Just to give you a practical example, imagine you have to optimize a png image available in an S3 bucket and save the resulting image in a new bucket.</p>\n<p>To optimize an image we can use <a href=\"https://github.com/imagemin/imagemin-cli\"><code>imagemin</code></a> which accepts an image in the standard input and returns the optimized image content through the standard output.</p>\n<p>Assuming we have our source image in <code>s3://my-images/image.png</code> and we want to save the optimized version in <code>s3://my-images-optimized/image.png</code>, we can write the pipeline as follows:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">aws s3 <span class=\"token function\">cp</span> s3://my-images/image.png - <span class=\"token operator\">|</span> imagemin <span class=\"token operator\">|</span> aws s3 <span class=\"token function\">cp</span> - s3://my-images-optimized/image.png</code></pre></div>\n<p>What will happen behind the scene with this pipeline of commands is the following:</p>\n<ol>\n<li>S3 will start to stream the binary content of <code>s3://my-images/image.png</code> to the standard output</li>\n<li>The standard output is then piped to <code>imagemin</code> and used as input stream</li>\n<li><code>imagemin</code> will start immediately to process the stream and produce an output stream representing the optimized image</li>\n<li>This output stream is then piped to the AWS CLI again and the <code>s3 cp</code> command will start to write it to the destination bucket.</li>\n</ol>\n<p>No intermediary file is created in the executing machine and the content is just kept in memory in a streaming fashion during the different phases of the pipeline.</p>\n<h2 id=\"the-5gb-caveat\" style=\"position:relative;\"><a href=\"#the-5gb-caveat\" aria-label=\"the 5gb caveat permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>The 5GB caveat</h2>\n<p>If you are writing to S3 files that are bigger than 5GB, you have to use the <code>--expected-size</code> option so that AWS CLI can calculate the proper number of parts in the multi-part upload. If you don’t do this you’ll exceed the number of parts allowed in a multi-part upload and your request will fail.</p>\n<p>From the <a href=\"https://docs.aws.amazon.com/cli/latest/reference/s3/cp.html\">AWS CLI Documentation</a>:</p>\n<blockquote>\n<p><code>--expected-size</code> (string): This argument specifies the expected size of a stream in terms of bytes. Note that this argument is needed only when a stream is being uploaded to s3 and the size is larger than 5GB. Failure to include this argument under these conditions may result in a failed upload due to too many parts in the upload.</p>\n</blockquote>\n<p><code>--expected-size</code> should be equal or greater than the size of the upload and it doesn’t have to be perfect. Just close enough.</p>\n<p>(Thanks to mahinka for this suggestion)</p>\n<h2 id=\"thats-all-folks\" style=\"position:relative;\"><a href=\"#thats-all-folks\" aria-label=\"thats all folks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>That’s all folks</h2>\n<p>I hope this little trick is going be useful to you and that it will allow you to use S3 in a much similar way to how you would use a local file system.</p>\n<p>I am really curious to know what kind of use cases you might come up with, so please, let me know in the comments here if you’ll ever use the nifty <code>-</code> option in the <code>aws s3 cp</code> command line utility.</p>\n<p>I really look forward to hearing from you!</p>\n<p>Special thanks to Paul for making me discover this trick and to <a href=\"https://www.reddit.com/user/mahinka\">mahinka</a> and <a href=\"https://www.reddit.com/user/paul345\">paul345</a> (on <a href=\"https://www.reddit.com/r/aws/comments/8h73uf/aws_command_line_s3_content_from_stdin_or_to/\">Reddit</a>) for corrections and suggestions.</p>\n<p>Until next time, ciao! 👋</p>","frontmatter":{"title":"AWS Command line: S3 content from stdin or to stdout","meta_description":null,"slug":"aws-command-line-s3-content-from-stdin-or-to-stdout","author":"Luciano Mammino","tags":["aws","bash","shell"],"date":"May 05, 2018","dateISO":"2018-05-05T12:00:14.000Z","header_img":{"publicURL":"/static/5a8d5e659e9cbbaf35101f4871e4f445/aws-command-line-s3-content-from-stdin-or-to-stdout-loige-co-luciano-mammino.jpg"},"fb_img":{"publicURL":"/static/a05826bfe520f88f114ebd7f7e1081bc/aws-command-line-s3-content-from-stdin-or-to-stdout-fb.png"},"tw_img":{"publicURL":"/static/48ebca8571e76cb8e79249b5894734dd/aws-command-line-s3-content-from-stdin-or-to-stdout-tw.png"},"written_with":null}}},"pageContext":{"tags":["aws","bash","shell"],"slug":"aws-command-line-s3-content-from-stdin-or-to-stdout","previous":{"timeToRead":14,"excerpt":"2017 is over and it’s time to sit down and see what happened during this year. As I already did last year, I would love to write a (potentially boring) recap of all the good and bad things (mostly from a career perspective) that happened in my life during this year. Last year I promised myself I would keep doing this as a way to keep track and benchmark my productivity over the coming years, so here I am to see how I performed! The year of conference talks! Let’s start with a big win! This was the year of…","fields":{"slug":"2017-a-year-in-review"},"frontmatter":{"date":"03 January, 2018","title":"2017 - A year in review","tags":["life"],"header_img":{"publicURL":"/static/964d2bbf2135e723a4ebbb84242a349f/loige-co-2017-a-year-in-review-luciano-mammino-blog.jpg"}}},"next":{"timeToRead":20,"excerpt":"In this article, I will share some of my notes and tips that might be useful if you are studying to get the AWS Solution Architect Associate Certification. I recently took this certification and I have to admit it was a little bit more challenging than I originally expected. I have been using a variety of AWS services professionally in the last 3 years, so I was optimistically expecting this practical experience to be enough. In reality, I had to spend some time to study and fill some gaps about important…","fields":{"slug":"aws-solution-architect-associate-exam-notes-tips"},"frontmatter":{"date":"21 October, 2018","title":"AWS Solution Architect Associate exam, my notes and tips","tags":["aws"],"header_img":{"publicURL":"/static/9f3b3348d4b78e8768f5f47770f9891d/aws-solution-architect-associate-exam-notes-tips.jpg"}}},"similar":[{"slug":"random-emoji-in-your-prompt-how-and-why","title":"A random emoji in your terminal prompt. How and Why!","publishedAt":"17 December, 2018","score":2},{"slug":"extracting-data-from-wikipedia-using-curl-grep-cut-and-other-bash-commands","title":"Extracting data from Wikipedia using curl, grep, cut and other shell commands","publishedAt":"15 August, 2016","score":2},{"slug":"aws-solution-architect-associate-exam-notes-tips","title":"AWS Solution Architect Associate exam, my notes and tips","publishedAt":"21 October, 2018","score":1},{"slug":"from-bare-metal-to-serverless","title":"From bare metal to Serverless","publishedAt":"16 December, 2017","score":1},{"slug":"using-lets-encrypt-and-certbot-to-automate-the-creation-of-certificates-for-openvpn","title":"Using Let’s Encrypt and Certbot to automate the creation of certificates for OpenVPN","publishedAt":"19 June, 2017","score":1}]}},"staticQueryHashes":[]}